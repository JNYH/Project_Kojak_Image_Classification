{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Image classification using pre-trained image model VGG16"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "State-of-the-art deep learning image classifiers (Pre-trained Image Models) are fully integrated into the Keras core\n",
    "Keras has five Convolutional Neural Networks that have been pre-trained on the ImageNet dataset:\n",
    "* VGG16 (visual geometry group, by Oxford)\n",
    "* VGG19\n",
    "* ResNet50\n",
    "* Inception V3 (by Google)\n",
    "* Xception\n",
    "\n",
    "Googles Goggles is the beginning of visual search technology.\n",
    "With this image recognition app, users can take a photo of a physical object, and Google will try to find information about what is pictured.\n",
    "\n",
    "Take a photo of a landmark and Google Goggles can give you its history.\n",
    "Snap a pic of a foreign menu, and it can be translated. \n",
    "the app can recognise and generate informaation on books, CDs, virtually anything that is 2D.\n",
    "\n",
    "business value:\n",
    "* another avenue to generate search data\n",
    "* recommend users to advertisers and retailers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "![](img/vgg16_croped.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "University of Oxford Visual Geometry Group has developed VGG16 trained weights [(details here)](https://github.com/fchollet/deep-learning-models/releases)\n",
    "\n",
    "Download the tensorflow h5 file [vgg16_weights_tf_dim_ordering_tf_kernels.h5](https://github.com/fchollet/deep-learning-models/releases/download/v0.1/vgg16_weights_tf_dim_ordering_tf_kernels.h5), and save it in the same directory as this notebook.\n",
    "\n",
    "Note this file is a little over half a gigabyte, so it will take a while to download.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # check that the above weight file is in the same directory as this notebook\n",
    "\n",
    "# weight_file = 'vgg16_weights_tf_dim_ordering_tf_kernels.h5'\n",
    "\n",
    "# import os\n",
    "# if not os.path.exists(weight_file):\n",
    "#     raise FileNotFoundError(\"No file {} found. Check path again\".format(weight_file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "\n",
      "  0     0    0     0    0     0      0      0 --:--:-- --:--:-- --:--:--     0\n",
      "100 31675  100 31675    0     0  92346      0 --:--:-- --:--:-- --:--:-- 92346\n"
     ]
    }
   ],
   "source": [
    "# # Download labels for VGG16\n",
    "# !curl https://raw.githubusercontent.com/torch/tutorials/master/7_imagenet_classification/synset_words.txt -o synset_words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A Convolutional Neural Network (CNN) Architecture\n",
    "\n",
    "### Option 1: Manual method to define the VGG16 architecture\n",
    "\n",
    "VGG16 model has been trained on a large dataset from imagenet, ie, ~1.2 million training images with another 50,000 images for validation and 100,000 images for testing. It has taken a huge amount of gpu time/power and data to train this model, which can classify an input image into 1,000 separate object categories.\n",
    "\n",
    "Here are [more examples of keras transfer learning](https://keras.io/applications/) with modern pre-trained CNNs. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "# K.set_image_dim_ordering('th')   # alternative(A) some Python version works on this\n",
    "K.common.set_image_dim_ordering('th')   # alternative(B)\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Flatten, Dense, Dropout, Activation\n",
    "from keras.layers.convolutional import Conv2D, MaxPooling2D, ZeroPadding2D\n",
    "from keras.optimizers import SGD\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PIL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This network is characterized by its simplicity, defined manually below:\n",
    "# using only 3×3 convolutional layers stacked on top of each other in increasing depth\n",
    "# Reducing volume size is handled by max pooling\n",
    "# Two fully-connected layers, each with 4,096 nodes are then followed by a softmax classifier\n",
    "\n",
    "def VGG_16(weights_path=None):\n",
    "    model = Sequential()\n",
    "    model.add(ZeroPadding2D((1,1),input_shape=(3,224,224)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(128, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(256, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(ZeroPadding2D((1,1)))\n",
    "    model.add(Conv2D(512, (3, 3), activation='relu'))\n",
    "    model.add(MaxPooling2D((2,2), strides=(2,2)))\n",
    "\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(4096, activation='relu'))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Dense(1000, activation='softmax'))\n",
    "\n",
    "    if weights_path:\n",
    "        model.load_weights(weights_path)\n",
    "\n",
    "    return model\n",
    "\n",
    "# define and compile model\n",
    "model = VGG_16(weight_file)   # note that we don't actually train/adjust the weights at all here\n",
    "sgd = SGD(lr=0.1, decay=1e-6, momentum=0.9, nesterov=True)\n",
    "model.compile(optimizer=sgd, loss='categorical_crossentropy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>synset</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>n01440764 tench, Tinca tinca</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>n01443537 goldfish, Carassius auratus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>n01484850 great white shark, white shark, man-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>n01491361 tiger shark, Galeocerdo cuvieri</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>n01494475 hammerhead, hammerhead shark</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>n01496331 electric ray, crampfish, numbfish, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>n01498041 stingray</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n01514668 cock</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>n01514859 hen</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>n01518878 ostrich, Struthio camelus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>n01530575 brambling, Fringilla montifringilla</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>n01531178 goldfinch, Carduelis carduelis</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>n01532829 house finch, linnet, Carpodacus mexi...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>n01534433 junco, snowbird</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>n01537544 indigo bunting, indigo finch, indigo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>n01558993 robin, American robin, Turdus migrat...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>n01560419 bulbul</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>n01580077 jay</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>n01582220 magpie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>n01592084 chickadee</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>n01601694 water ouzel, dipper</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>n01608432 kite</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>n01614925 bald eagle, American eagle, Haliaeet...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>n01616318 vulture</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>n01622779 great grey owl, great gray owl, Stri...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>n01629819 European fire salamander, Salamandra...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>n01630670 common newt, Triturus vulgaris</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>n01631663 eft</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>n01632458 spotted salamander, Ambystoma maculatum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>n01632777 axolotl, mud puppy, Ambystoma mexicanum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>970</th>\n",
       "      <td>n09193705 alp</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>971</th>\n",
       "      <td>n09229709 bubble</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>972</th>\n",
       "      <td>n09246464 cliff, drop, drop-off</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>973</th>\n",
       "      <td>n09256479 coral reef</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>974</th>\n",
       "      <td>n09288635 geyser</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>975</th>\n",
       "      <td>n09332890 lakeside, lakeshore</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>976</th>\n",
       "      <td>n09399592 promontory, headland, head, foreland</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>977</th>\n",
       "      <td>n09421951 sandbar, sand bar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>978</th>\n",
       "      <td>n09428293 seashore, coast, seacoast, sea-coast</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>979</th>\n",
       "      <td>n09468604 valley, vale</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>980</th>\n",
       "      <td>n09472597 volcano</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>981</th>\n",
       "      <td>n09835506 ballplayer, baseball player</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>982</th>\n",
       "      <td>n10148035 groom, bridegroom</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>983</th>\n",
       "      <td>n10565667 scuba diver</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>984</th>\n",
       "      <td>n11879895 rapeseed</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>985</th>\n",
       "      <td>n11939491 daisy</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>986</th>\n",
       "      <td>n12057211 yellow lady's slipper, yellow lady-s...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>987</th>\n",
       "      <td>n12144580 corn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>988</th>\n",
       "      <td>n12267677 acorn</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>989</th>\n",
       "      <td>n12620546 hip, rose hip, rosehip</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>990</th>\n",
       "      <td>n12768682 buckeye, horse chestnut, conker</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>n12985857 coral fungus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>992</th>\n",
       "      <td>n12998815 agaric</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>993</th>\n",
       "      <td>n13037406 gyromitra</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>994</th>\n",
       "      <td>n13040303 stinkhorn, carrion fungus</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>n13044778 earthstar</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>n13052670 hen-of-the-woods, hen of the woods, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>n13054560 bolete</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>n13133613 ear, spike, capitulum</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>n15075141 toilet tissue, toilet paper, bathroo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                synset\n",
       "0                         n01440764 tench, Tinca tinca\n",
       "1                n01443537 goldfish, Carassius auratus\n",
       "2    n01484850 great white shark, white shark, man-...\n",
       "3            n01491361 tiger shark, Galeocerdo cuvieri\n",
       "4               n01494475 hammerhead, hammerhead shark\n",
       "5    n01496331 electric ray, crampfish, numbfish, t...\n",
       "6                                   n01498041 stingray\n",
       "7                                       n01514668 cock\n",
       "8                                        n01514859 hen\n",
       "9                  n01518878 ostrich, Struthio camelus\n",
       "10       n01530575 brambling, Fringilla montifringilla\n",
       "11            n01531178 goldfinch, Carduelis carduelis\n",
       "12   n01532829 house finch, linnet, Carpodacus mexi...\n",
       "13                           n01534433 junco, snowbird\n",
       "14   n01537544 indigo bunting, indigo finch, indigo...\n",
       "15   n01558993 robin, American robin, Turdus migrat...\n",
       "16                                    n01560419 bulbul\n",
       "17                                       n01580077 jay\n",
       "18                                    n01582220 magpie\n",
       "19                                 n01592084 chickadee\n",
       "20                       n01601694 water ouzel, dipper\n",
       "21                                      n01608432 kite\n",
       "22   n01614925 bald eagle, American eagle, Haliaeet...\n",
       "23                                   n01616318 vulture\n",
       "24   n01622779 great grey owl, great gray owl, Stri...\n",
       "25   n01629819 European fire salamander, Salamandra...\n",
       "26            n01630670 common newt, Triturus vulgaris\n",
       "27                                       n01631663 eft\n",
       "28   n01632458 spotted salamander, Ambystoma maculatum\n",
       "29   n01632777 axolotl, mud puppy, Ambystoma mexicanum\n",
       "..                                                 ...\n",
       "970                                      n09193705 alp\n",
       "971                                   n09229709 bubble\n",
       "972                    n09246464 cliff, drop, drop-off\n",
       "973                               n09256479 coral reef\n",
       "974                                   n09288635 geyser\n",
       "975                      n09332890 lakeside, lakeshore\n",
       "976     n09399592 promontory, headland, head, foreland\n",
       "977                        n09421951 sandbar, sand bar\n",
       "978     n09428293 seashore, coast, seacoast, sea-coast\n",
       "979                             n09468604 valley, vale\n",
       "980                                  n09472597 volcano\n",
       "981              n09835506 ballplayer, baseball player\n",
       "982                        n10148035 groom, bridegroom\n",
       "983                              n10565667 scuba diver\n",
       "984                                 n11879895 rapeseed\n",
       "985                                    n11939491 daisy\n",
       "986  n12057211 yellow lady's slipper, yellow lady-s...\n",
       "987                                     n12144580 corn\n",
       "988                                    n12267677 acorn\n",
       "989                   n12620546 hip, rose hip, rosehip\n",
       "990          n12768682 buckeye, horse chestnut, conker\n",
       "991                             n12985857 coral fungus\n",
       "992                                   n12998815 agaric\n",
       "993                                n13037406 gyromitra\n",
       "994                n13040303 stinkhorn, carrion fungus\n",
       "995                                n13044778 earthstar\n",
       "996  n13052670 hen-of-the-woods, hen of the woods, ...\n",
       "997                                   n13054560 bolete\n",
       "998                    n13133613 ear, spike, capitulum\n",
       "999  n15075141 toilet tissue, toilet paper, bathroo...\n",
       "\n",
       "[1000 rows x 1 columns]"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load labels for VGG16\n",
    "# synset = pd.read_csv('synset_words.txt', skipinitialspace=True, names = ['synset', 'words'])   # simplified classes/labels\n",
    "synset = pd.read_csv('synset_words.csv', skipinitialspace=True, names = ['synset'])   # full classification classes/labels\n",
    "synset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_image_to_bgr_numpy_array(image_path, size=(224,224)):\n",
    "    \"\"\"The network has been trained using opencv and BGR images \n",
    "    (i.e. channels order blue, green, red rather than red, green, blue).\n",
    "    The description of why is https://stackoverflow.com/questions/14556545/why-opencv-using-bgr-colour-space-instead-of-rgb\n",
    "    \n",
    "    We can use a simpler image library as long as we manually convert\n",
    "    the data to the expected format.\n",
    "    \"\"\"\n",
    "    image = PIL.Image.open(image_path).resize(size)\n",
    "    img_data = np.array(image.getdata(), np.float32).reshape(*size, -1)\n",
    "    # swap R and B channels\n",
    "    img_data = np.flip(img_data, axis=2)\n",
    "    return img_data\n",
    "\n",
    "def prepare_image(image_path):\n",
    "    im = convert_image_to_bgr_numpy_array(image_path)\n",
    "\n",
    "    # these subtractions are just mean centering the images based on known means for different color channels\n",
    "    im[:,:,0] -= 103.939\n",
    "    im[:,:,1] -= 116.779\n",
    "    im[:,:,2] -= 123.68\n",
    "\n",
    "    im = im.transpose((2,0,1))        # adjust from (224, 224, 3) to (3, 224, 224) for keras\n",
    "    im = np.expand_dims(im, axis=0)   # adjust to (1, 3, 224, 224) for generating keras prediction\n",
    "    return im"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "n02112018 Pomeranian\n"
     ]
    }
   ],
   "source": [
    "# make predictions using compiled VGG16 model\n",
    "img = prepare_image('img/dog.jpg')\n",
    "out = model.predict(img)\n",
    "y_pred = np.argmax(out)\n",
    "print(y_pred)\n",
    "print(synset.loc[y_pred].synset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1, 1000)"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Option 2: Loading VGG16 using keras utilities (recommended)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# highly recommended method, instead of manually defining architectures and loading weights\n",
    "from keras.applications.vgg16 import VGG16\n",
    "from keras.applications.imagenet_utils import decode_predictions\n",
    "\n",
    "model2 = VGG16()   # can this work without specifying the weights?\n",
    "# model2 = VGG16(weights='imagenet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "259\n",
      "n02112018 Pomeranian\n",
      "Predicted: [[('n02112018', 'Pomeranian', 0.5479653), ('n02113023', 'Pembroke', 0.11714048), ('n02115641', 'dingo', 0.07175173), ('n02085620', 'Chihuahua', 0.033746526), ('n02104365', 'schipperke', 0.030352084)]]\n"
     ]
    }
   ],
   "source": [
    "# make predictions using loaded VGG16 model\n",
    "img = prepare_image('img/dog.jpg')\n",
    "out2 = model2.predict(img)\n",
    "y_pred2 = np.argmax(out2)\n",
    "print(y_pred2)\n",
    "print(synset.loc[y_pred2].synset)\n",
    "print('Predicted:', decode_predictions(out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "235\n",
      "n02106662 German shepherd, German shepherd dog, German police dog, alsatian\n",
      "Predicted: [[('n02106662', 'German_shepherd', 0.9972421), ('n02105162', 'malinois', 0.0015698465), ('n03803284', 'muzzle', 0.00032143845), ('n04254680', 'soccer_ball', 0.00016304254), ('n02105412', 'kelpie', 0.00015440548)]]\n",
      "235\n",
      "n02106662 German shepherd, German shepherd dog, German police dog, alsatian\n",
      "Predicted: [[('n02106662', 'German_shepherd', 0.9972421), ('n02105162', 'malinois', 0.0015698465), ('n03803284', 'muzzle', 0.00032143845), ('n04254680', 'soccer_ball', 0.00016304254), ('n02105412', 'kelpie', 0.00015440548)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/dog_2.jpg')\n",
    "\n",
    "out = model.predict(img)\n",
    "y_pred = np.argmax(out)\n",
    "print(y_pred)\n",
    "print(synset.loc[y_pred].synset)\n",
    "print('Predicted:', decode_predictions(out))\n",
    "\n",
    "out2 = model2.predict(img)\n",
    "y_pred2 = np.argmax(out2)\n",
    "print(y_pred2)\n",
    "print(synset.loc[y_pred2].synset)\n",
    "print('Predicted:', decode_predictions(out2))\n",
    "\n",
    "# notice both model and model2 outputs are the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n04350905', 'suit', 0.7036784), ('n04591157', 'Windsor_tie', 0.16369325), ('n03838899', 'oboe', 0.026618008), ('n10148035', 'groom', 0.013540737), ('n02883205', 'bow_tie', 0.011791961)]]\n",
      "Predicted: [[('n04350905', 'suit', 0.7036784), ('n04591157', 'Windsor_tie', 0.16369325), ('n03838899', 'oboe', 0.026618008), ('n10148035', 'groom', 0.013540737), ('n02883205', 'bow_tie', 0.011791961)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/test.jpg')\n",
    "out = model.predict(img)\n",
    "print('Predicted:', decode_predictions(out))\n",
    "out2 = model2.predict(img)\n",
    "print('Predicted:', decode_predictions(out2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n07930864 cup\n",
      "Predicted: [[('n07930864', 'cup', 0.6994144), ('n03063599', 'coffee_mug', 0.18904433), ('n04131690', 'saltshaker', 0.020779125), ('n03063689', 'coffeepot', 0.011247833), ('n04423845', 'thimble', 0.0071213855)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/sloth.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02457408 three-toed sloth, ai, Bradypus tridactylus\n",
      "Predicted: [[('n02457408', 'three-toed_sloth', 0.98550665), ('n02483362', 'gibbon', 0.0010795437), ('n01622779', 'great_grey_owl', 0.0007896442), ('n02493509', 'titi', 0.0006946715), ('n02500267', 'indri', 0.00065417995)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/sloth2.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02457408 three-toed sloth, ai, Bradypus tridactylus\n",
      "Predicted: [[('n02457408', 'three-toed_sloth', 0.9983479), ('n02493509', 'titi', 0.00095363497), ('n02483362', 'gibbon', 0.00019791185), ('n02138441', 'meerkat', 0.00017710007), ('n02490219', 'marmoset', 8.666833e-05)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/sloth3.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02098286 West Highland white terrier\n",
      "Predicted: [[('n02098286', 'West_Highland_white_terrier', 0.6178925), ('n02085936', 'Maltese_dog', 0.12265609), ('n02094114', 'Norfolk_terrier', 0.07862637), ('n02096177', 'cairn', 0.03849569), ('n02094433', 'Yorkshire_terrier', 0.032624725)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Dog_3.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n02085936 Maltese dog, Maltese terrier, Maltese\n",
      "Predicted: [[('n02085936', 'Maltese_dog', 0.9560953), ('n02098413', 'Lhasa', 0.018933775), ('n02086240', 'Shih-Tzu', 0.009139382), ('n02086079', 'Pekinese', 0.0061969035), ('n02113624', 'toy_poodle', 0.003968405)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/Dog_4.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07745940', 'strawberry', 0.99977857), ('n04332243', 'strainer', 3.446263e-05), ('n07747607', 'orange', 2.406561e-05), ('n07753592', 'banana', 2.256647e-05), ('n07768694', 'pomegranate', 2.1113809e-05)]]\n",
      "Predicted: [[('n07745940', 'strawberry', 0.99977857), ('n04332243', 'strainer', 3.446263e-05), ('n07747607', 'orange', 2.406561e-05), ('n07753592', 'banana', 2.256647e-05), ('n07768694', 'pomegranate', 2.1113809e-05)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/strawberry.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07614500', 'ice_cream', 0.20130746), ('n03476684', 'hair_slide', 0.057905596), ('n07579787', 'plate', 0.057216298), ('n07745940', 'strawberry', 0.054555055), ('n07714571', 'head_cabbage', 0.050523743)]]\n",
      "Predicted: [[('n07614500', 'ice_cream', 0.20130746), ('n03476684', 'hair_slide', 0.057905596), ('n07579787', 'plate', 0.057216298), ('n07745940', 'strawberry', 0.054555055), ('n07714571', 'head_cabbage', 0.050523743)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/icecream.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n07614500', 'ice_cream', 0.87625885), ('n07613480', 'trifle', 0.11704696), ('n07836838', 'chocolate_sauce', 0.004256764), ('n07745940', 'strawberry', 0.0008514527), ('n07579787', 'plate', 0.0003462588)]]\n",
      "Predicted: [[('n07614500', 'ice_cream', 0.87625885), ('n07613480', 'trifle', 0.11704696), ('n07836838', 'chocolate_sauce', 0.004256764), ('n07745940', 'strawberry', 0.0008514527), ('n07579787', 'plate', 0.0003462588)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/icecream2.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02097047', 'miniature_schnauzer', 0.7287519), ('n02097209', 'standard_schnauzer', 0.24815588), ('n02097130', 'giant_schnauzer', 0.020994069), ('n02093991', 'Irish_terrier', 0.0005178273), ('n02096051', 'Airedale', 0.0003147888)]]\n",
      "Predicted: [[('n02097047', 'miniature_schnauzer', 0.7287519), ('n02097209', 'standard_schnauzer', 0.24815588), ('n02097130', 'giant_schnauzer', 0.020994069), ('n02093991', 'Irish_terrier', 0.0005178273), ('n02096051', 'Airedale', 0.0003147888)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/schnauzer.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02085936', 'Maltese_dog', 0.61389536), ('n02086240', 'Shih-Tzu', 0.10318202), ('n02098413', 'Lhasa', 0.09179912), ('n02086079', 'Pekinese', 0.069826424), ('n02113624', 'toy_poodle', 0.017381951)]]\n",
      "Predicted: [[('n02085936', 'Maltese_dog', 0.61389536), ('n02086240', 'Shih-Tzu', 0.10318202), ('n02098413', 'Lhasa', 0.09179912), ('n02086079', 'Pekinese', 0.069826424), ('n02113624', 'toy_poodle', 0.017381951)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/fan_sil1.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n04162706', 'seat_belt', 0.7091236), ('n02085936', 'Maltese_dog', 0.21847743), ('n02098286', 'West_Highland_white_terrier', 0.040213455), ('n02098413', 'Lhasa', 0.009687599), ('n02094114', 'Norfolk_terrier', 0.002983844)]]\n",
      "Predicted: [[('n04162706', 'seat_belt', 0.7091236), ('n02085936', 'Maltese_dog', 0.21847743), ('n02098286', 'West_Highland_white_terrier', 0.040213455), ('n02098413', 'Lhasa', 0.009687599), ('n02094114', 'Norfolk_terrier', 0.002983844)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/fan_sil2.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted: [[('n02085936', 'Maltese_dog', 0.8535949), ('n02098413', 'Lhasa', 0.064508125), ('n02098286', 'West_Highland_white_terrier', 0.04744286), ('n02086240', 'Shih-Tzu', 0.0124242455), ('n04162706', 'seat_belt', 0.0057937186)]]\n",
      "Predicted: [[('n02085936', 'Maltese_dog', 0.8535949), ('n02098413', 'Lhasa', 0.064508125), ('n02098286', 'West_Highland_white_terrier', 0.04744286), ('n02086240', 'Shih-Tzu', 0.0124242455), ('n04162706', 'seat_belt', 0.0057937186)]]\n"
     ]
    }
   ],
   "source": [
    "img = prepare_image('img/fan_sil3.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/ddog_basset_hound.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/ddog_beagle.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/ddog_english_foxhound.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/ddog_walker_hound.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/poodle_miniature.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/poodle_standard.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/poodle_toy.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/cat_egyptian_cat.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/cat_persian.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/cat_siamese_cat.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/cat_tabby_cat_mackerel.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/cat_tiger_cat.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_cheetah.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_cougar.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_jaguar.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_leopard.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_linx_cat.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/catt_snow_leopard.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/guinea_pig_abyssinian.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/guinea_pig_american1.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/guinea_pig_himalayan.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/guinea_pig_silkie-hazelnut.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/guinea_pig_skinny.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_black_grouse.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_bulbul.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_chickadee.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_coucal.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_goldfinch.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_hornbill.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_indigo_bunting.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_jay.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_magpie.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_partridge.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_ptarmigan.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img = prepare_image('img/bird_quail.jpg')\n",
    "out = model.predict(img)\n",
    "print(synset.loc[np.argmax(out)].synset)\n",
    "print('Predicted:', decode_predictions(out))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Transfer Learning\n",
    "\n",
    "it turns out that the lower level featured learned by VGG16 on imagenet are still applicable to other problems with natural images. If we can preserve the lower-level features, we can just train a new model on those features. (In fact, in the case of 'softmax', we can think of this as just training a new multinomial logistic regression, on those convolution features)\n",
    "\n",
    "Lets just snip off last layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "A Caveat\n",
    "\n",
    "if we just add a new layer with default weights, it is going to be very wrong the first iteration. Since it is so wrong, the gradient will be huge, and because we are using back propagation those errors will be sent down stream into the lower level features. This can quickly destroy the rest of the network.\n",
    "\n",
    "In order to retrain this model we must protect the lower-level features, until our new layers have reached more stability. We can do this by freezing those layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then we'll add our new layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from keras.models import Model\n",
    "\n",
    "# base_model = VGG16(weights='imagenet', include_top=False, input_shape=(3,224,224)) \n",
    "# # Freeze convolutional layers\n",
    "# for layer in base_model.layers:\n",
    "#     layer.trainable = False \n",
    "\n",
    "# #     model.add(Flatten())\n",
    "# #     model.add(Dense(4096, activation='relu'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "# #     model.add(Dense(4096, activation='relu'))\n",
    "# #     model.add(Dropout(0.5))\n",
    "# #     model.add(Dense(1000, activation='softmax'))\n",
    "# # note we exclude the above final dense layers, and add the dense layers below, so we could retrain it ourselves\n",
    "\n",
    "# x = base_model.output\n",
    "# x = Flatten()(x) # flatten from convolution tensor output \n",
    "# x = Dense(512, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# x = Dense(256, activation='relu')(x)\n",
    "# x = Dropout(0.5)(x)\n",
    "# predictions = Dense(3, activation='softmax')(x) # should match # of classes predicted\n",
    "\n",
    "# # this is the model we will train\n",
    "# model = Model(inputs=base_model.input, outputs=predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<keras.engine.input_layer.InputLayer at 0x2b38182f710>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b38182f780>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b38182f908>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2b380819518>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b380819668>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b380505a20>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2b3805eb898>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b3805eb9e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b38077ed68>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b38066cba8>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2b3805ad9e8>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b3805adb38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b38062dd30>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b381812c88>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2b381887b38>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b381887c88>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b3806b0080>,\n",
       " <keras.layers.convolutional.Conv2D at 0x2b380692f60>,\n",
       " <keras.layers.pooling.MaxPooling2D at 0x2b380559eb8>]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'block5_pool_1/transpose_1:0' shape=(?, 512, 7, 7) dtype=float32>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "base_model.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'dropout_4/cond/Merge:0' shape=(?, 256) dtype=float32>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "model.compile(optimizer=SGD(lr=0.0001, momentum=0.9),\n",
    "            loss='categorical_crossentropy', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Then you would just train like normal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "```python\n",
    "# i.e. if we had training images and our own labels, we could run\n",
    "model.fit(X_train,y_train)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "How much data do you need?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "More!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Actually with this bottleneck approach, you don't need as much. 200-1000 representitive images of each class will give good results. Because\n",
    "* Google has already done most of the hard work\n",
    "* We can use image augmentation to increase our number of training samples"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "New Architectures are being published every day. So much to read!\n",
    "\n",
    "* [Curated List of Deep Learning papers](https://github.com/ChristosChristofidis/awesome-deep-learning)\n",
    "* [Good reddit post for keeping up with the latest research](https://www.reddit.com/r/MachineLearning/comments/6d7nb1/d_machine_learning_wayr_what_are_you_reading_week/)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "metis",
   "language": "python",
   "name": "metis"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
